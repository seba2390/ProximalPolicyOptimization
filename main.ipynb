{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-08T17:31:37.811774Z",
     "start_time": "2023-11-08T17:31:37.792126Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.Agent import PPOAgent\n",
    "from src.OptimizerParameters import AdamOptimizerParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Testing architecture w. individual Policy and Value net:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26887cc148ce6001"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# General Hyperparams\n",
    "gamma = 0.99\n",
    "lmbda = 0.925\n",
    "epsilon = 0.2  \n",
    "num_policy_epochs = 5\n",
    "num_value_epochs = 5\n",
    "num_multihead_epochs = 5\n",
    "smoothing_const = 1e-8\n",
    "normalize_advantages = True\n",
    "batch_size = 32\n",
    "shuffle_batches = False\n",
    "max_game_length = 500\n",
    "architecture = \"Individual Networks\"\n",
    "num_episodes = 40\n",
    "\n",
    "# Optimizer Hyperparams\n",
    "policy_optimizer_parameters = AdamOptimizerParameters(lr=0.0025, betas=(0.9, 0.999), weight_decay=0)\n",
    "value_optimizer_parameters = AdamOptimizerParameters(lr=0.0025, betas=(0.9, 0.999), weight_decay=0)\n",
    "multihead_optimizer_parameters = AdamOptimizerParameters(lr=0.0025, betas=(0.9, 0.999), weight_decay=0)\n",
    "\n",
    "\n",
    "rng_seed = 0\n",
    "env = gym.make(id='CartPole-v1', max_episode_steps=max_game_length)\n",
    "\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "\n",
    "Agent = PPOAgent(env=env,\n",
    "                 state_space_size=input_dim,\n",
    "                 action_space_size=output_dim,\n",
    "                 gamma=gamma,\n",
    "                 lmbda=lmbda,\n",
    "                 epsilon=epsilon,\n",
    "                 smooting_const=smoothing_const,\n",
    "                 normalize_advantages=normalize_advantages,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle_batches = shuffle_batches,\n",
    "                 architecture=architecture,\n",
    "                 seed=rng_seed)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T17:31:37.822808Z",
     "start_time": "2023-11-08T17:31:37.798463Z"
    }
   },
   "id": "7453114a18179ff3"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################################\n",
      "# --- Survived for: 11 episodes, and earned a total reward of: 11.0 --- #\n",
      "#########################################################################\n"
     ]
    }
   ],
   "source": [
    "# Trying to interact w. env. before training\n",
    "Agent.play()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T17:31:37.836179Z",
     "start_time": "2023-11-08T17:31:37.803800Z"
    }
   },
   "id": "806da1ca6de0d5e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [00:02<00:07,  3.26it/s]"
     ]
    }
   ],
   "source": [
    "if architecture == \"Individual Networks\":\n",
    "    avg_accumulated_rewards, avg_value_net_loss, avg_policy_net_loss = Agent.train(episodes=num_episodes,\n",
    "                                                                                   num_policy_epochs=num_policy_epochs,\n",
    "                                                                                   num_value_epochs=num_value_epochs,\n",
    "                                                                                   policy_optimizer_params=policy_optimizer_parameters,\n",
    "                                                                                   value_optimizer_params=value_optimizer_parameters)\n",
    "elif architecture == \"Multi Head Network\":\n",
    "    avg_accumulated_rewards, avg_multihead_net_loss = Agent.train(episodes=num_episodes,\n",
    "                                                                  num_multihead_epochs=num_multihead_epochs,\n",
    "                                                                  multihead_optimizer_params=multihead_optimizer_parameters)\n",
    "else:\n",
    "    pass\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-08T17:31:37.810340Z"
    }
   },
   "id": "3d5da7a8366ef4a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "ax[0].plot([e for e in range(len(avg_accumulated_rewards))], avg_accumulated_rewards)\n",
    "ax[0].set_xlabel('Episodes')\n",
    "ax[0].set_ylabel('Avg. Accumulated Reward')\n",
    "\n",
    "ax[1].plot([e for e in range(len(avg_accumulated_rewards))], avg_accumulated_rewards)\n",
    "ax[1].set_xlabel('Episodes')\n",
    "ax[1].set_ylabel('Avg. Accumulated Reward [log]')\n",
    "ax[1].set_yscale('log')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7c16c60286408ffb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Trying to interact w. env. after training\n",
    "Agent.play()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ef1be68ab58d878f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Testing architecture w. common backbone network w. Policy and Value heads:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fb2865a3a6e8ec7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "architecture = \"Multi Head Network\"\n",
    "\n",
    "\n",
    "Agent = PPOAgent(env=env,\n",
    "                 state_space_size=input_dim,\n",
    "                 action_space_size=output_dim,\n",
    "                 gamma=gamma,\n",
    "                 lmbda=lmbda,\n",
    "                 epsilon=epsilon,\n",
    "                 smooting_const=smoothing_const,\n",
    "                 normalize_advantages=normalize_advantages,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle_batches = shuffle_batches,\n",
    "                 architecture=architecture,\n",
    "                 seed=rng_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5f657b130f7bfd78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Trying to interact w. env. before training\n",
    "Agent.play()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "32b5d84ee6aa35bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if architecture == \"Individual Networks\":\n",
    "    avg_accumulated_rewards, avg_value_net_loss, avg_policy_net_loss = Agent.train(episodes=num_episodes,\n",
    "                                                                                   num_policy_epochs=num_policy_epochs,\n",
    "                                                                                   num_value_epochs=num_value_epochs,\n",
    "                                                                                   policy_optimizer_params=policy_optimizer_parameters,\n",
    "                                                                                   value_optimizer_params=value_optimizer_parameters)\n",
    "elif architecture == \"Multi Head Network\":\n",
    "    avg_accumulated_rewards, avg_multihead_net_loss = Agent.train(episodes=num_episodes,\n",
    "                                                                  num_multihead_epochs=num_multihead_epochs,\n",
    "                                                                  multihead_optimizer_params=multihead_optimizer_parameters)\n",
    "else:\n",
    "    pass\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "32497dac59aa5e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "ax[0].plot([e for e in range(len(avg_accumulated_rewards))], avg_accumulated_rewards)\n",
    "ax[0].set_xlabel('Episodes')\n",
    "ax[0].set_ylabel('Avg. Accumulated Reward')\n",
    "\n",
    "ax[1].plot([e for e in range(len(avg_accumulated_rewards))], avg_accumulated_rewards)\n",
    "ax[1].set_xlabel('Episodes')\n",
    "ax[1].set_ylabel('Avg. Accumulated Reward [log]')\n",
    "ax[1].set_yscale('log')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f0173c9d3592e585"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Trying to interact w. env. after training\n",
    "Agent.play()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "773447a5a5375106"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d61083557b84e12a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
