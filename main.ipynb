{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-08T00:01:28.792363Z",
     "start_time": "2023-11-08T00:01:28.770941Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.Agent import PPOAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Testing architecture w. individual Policy and Value net:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26887cc148ce6001"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "rng_seed = 0\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "gamma = 0.99\n",
    "lmbda = 0.925\n",
    "epsilon = 0.2  \n",
    "policy_lr = 0.001\n",
    "value_lr = 0.001\n",
    "multihead_lr = 0.001\n",
    "num_policy_epochs = 5\n",
    "num_value_epochs = 5\n",
    "num_multihead_epochs = 5\n",
    "smoothing_const = 1e-8\n",
    "normalize_advantages = True\n",
    "batch_size = 32\n",
    "shuffle_batches = False\n",
    "max_game_length = 500\n",
    "architecture = \"Individual Networks\"\n",
    "num_episodes = 1000\n",
    "\n",
    "Agent = PPOAgent(env=env,\n",
    "                 state_space_size=input_dim,\n",
    "                 action_space_size=output_dim,\n",
    "                 gamma=gamma,\n",
    "                 lmbda=lmbda,\n",
    "                 epsilon=epsilon,\n",
    "                 smooting_const=smoothing_const,\n",
    "                 normalize_advantages=normalize_advantages,\n",
    "                 batch_size=batch_size,\n",
    "                 max_game_length=max_game_length,\n",
    "                 shuffle_batches = shuffle_batches,\n",
    "                 architecture=architecture,\n",
    "                 seed=rng_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T00:01:28.806231Z",
     "start_time": "2023-11-08T00:01:28.779208Z"
    }
   },
   "id": "7453114a18179ff3"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "# --- Survived for: 11 episodes --- #\n",
      "####################################\n"
     ]
    }
   ],
   "source": [
    "# Trying to interact w. env. before training\n",
    "Agent.play(render=False, max_game_length=max_game_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T00:01:28.818201Z",
     "start_time": "2023-11-08T00:01:28.784955Z"
    }
   },
   "id": "806da1ca6de0d5e6"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–Ž         | 25/1000 [00:05<03:27,  4.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m architecture \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIndividual Networks\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m----> 2\u001B[0m     avg_accumulated_rewards, avg_value_net_loss, avg_policy_net_loss \u001B[38;5;241m=\u001B[39m \u001B[43mAgent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepisodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_episodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                                                                                   \u001B[49m\u001B[43mpolicy_lr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_lr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                                                                                   \u001B[49m\u001B[43mvalue_lr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalue_lr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                                                                                   \u001B[49m\u001B[43mnum_policy_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_policy_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                                                                                   \u001B[49m\u001B[43mnum_value_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_value_epochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m architecture \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMulti Head Network\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m      8\u001B[0m     avg_accumulated_rewards, avg_multihead_net_loss \u001B[38;5;241m=\u001B[39m Agent\u001B[38;5;241m.\u001B[39mtrain(episodes\u001B[38;5;241m=\u001B[39mnum_episodes,\n\u001B[1;32m      9\u001B[0m                                                                   multihead_lr\u001B[38;5;241m=\u001B[39mmultihead_lr,\n\u001B[1;32m     10\u001B[0m                                                                   num_multihead_epochs\u001B[38;5;241m=\u001B[39mnum_multihead_epochs)\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-AarhusUniversitet/Div. Github projekter/Privat/ProximalPolicyOptimization/src/Agent.py:220\u001B[0m, in \u001B[0;36mPPOAgent.train\u001B[0;34m(self, episodes, policy_lr, value_lr, multihead_lr, num_policy_epochs, num_value_epochs, num_multihead_epochs)\u001B[0m\n\u001B[1;32m    208\u001B[0m trajectories \u001B[38;5;241m=\u001B[39m Trajectories(batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[1;32m    209\u001B[0m                             max_game_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_game_length,\n\u001B[1;32m    210\u001B[0m                             env\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    213\u001B[0m                             architecture\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39marchitecture,\n\u001B[1;32m    214\u001B[0m                             seed\u001B[38;5;241m=\u001B[39mepisode \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    216\u001B[0m \u001B[38;5;66;03m# States_batch has dims:      (batch size , game length , state space size)\u001B[39;00m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;66;03m# next_states_batch has dims: (batch size , game length , state space size)\u001B[39;00m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;66;03m# actions_batch has dims:     (batch size , game length)\u001B[39;00m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;66;03m# rewards_batch has dims:     (batch size , game length)\u001B[39;00m\n\u001B[0;32m--> 220\u001B[0m states_batch, actions_batch, rewards_batch, next_states_batch \u001B[38;5;241m=\u001B[39m \u001B[43mtrajectories\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    221\u001B[0m avg_accumulated_reward\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39mmean([torch\u001B[38;5;241m.\u001B[39msum(rewards)\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mfor\u001B[39;00m rewards \u001B[38;5;129;01min\u001B[39;00m rewards_batch]))\n\u001B[1;32m    223\u001B[0m \u001B[38;5;66;03m# Computing advantages\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-AarhusUniversitet/Div. Github projekter/Privat/ProximalPolicyOptimization/src/Trajectories.py:76\u001B[0m, in \u001B[0;36mTrajectories.get_batch\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     74\u001B[0m states_batch, actions_batch, rewards_batch, next_states_batch \u001B[38;5;241m=\u001B[39m [], [], [], []\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m datapoint \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size):\n\u001B[0;32m---> 76\u001B[0m     states, actions, rewards, next_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_trajectory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdatapoint\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m     states_batch\u001B[38;5;241m.\u001B[39mappend(torch\u001B[38;5;241m.\u001B[39mtensor(states))\n\u001B[1;32m     78\u001B[0m     actions_batch\u001B[38;5;241m.\u001B[39mappend(torch\u001B[38;5;241m.\u001B[39mtensor(actions))\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-AarhusUniversitet/Div. Github projekter/Privat/ProximalPolicyOptimization/src/Trajectories.py:43\u001B[0m, in \u001B[0;36mTrajectories.get_trajectory\u001B[0;34m(self, seed)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(rewards) \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_game_length:\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;66;03m# Retrieve current action prob. distribution\u001B[39;00m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39marchitecture \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIndividual Networks\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 43\u001B[0m         action_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39marchitecture \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMulti Head Network\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     45\u001B[0m         _, action_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmultihead_network(torch\u001B[38;5;241m.\u001B[39mtensor(state))\n",
      "File \u001B[0;32m~/.python_venvs/PPO/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.python_venvs/PPO/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-AarhusUniversitet/Div. Github projekter/Privat/ProximalPolicyOptimization/src/PolicyNetwork.py:35\u001B[0m, in \u001B[0;36mPolicyNetwork.forward\u001B[0;34m(self, state)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, state: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m---> 35\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivtion_1(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlin_layer_1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     36\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivtion_2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlin_layer_2(out))\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/.python_venvs/PPO/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.python_venvs/PPO/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.python_venvs/PPO/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if architecture == \"Individual Networks\":\n",
    "    avg_accumulated_rewards, avg_value_net_loss, avg_policy_net_loss = Agent.train(episodes=num_episodes,\n",
    "                                                                                   policy_lr=policy_lr,\n",
    "                                                                                   value_lr=value_lr,\n",
    "                                                                                   num_policy_epochs=num_policy_epochs,\n",
    "                                                                                   num_value_epochs=num_value_epochs)\n",
    "elif architecture == \"Multi Head Network\":\n",
    "    avg_accumulated_rewards, avg_multihead_net_loss = Agent.train(episodes=num_episodes,\n",
    "                                                                  multihead_lr=multihead_lr,\n",
    "                                                                  num_multihead_epochs=num_multihead_epochs)\n",
    "else:\n",
    "    pass\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T00:01:34.372146Z",
     "start_time": "2023-11-08T00:01:28.790928Z"
    }
   },
   "id": "3d5da7a8366ef4a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if architecture == \"Individual Networks\":\n",
    "    fig, ax = plt.subplots(3,2, figsize=(12,10))\n",
    "    ax[0][0].plot([e for e in range(len(avg_accumulated_rewards))], avg_accumulated_rewards)\n",
    "    ax[0][0].set_xlabel('Episodes')\n",
    "    ax[0][0].set_ylabel('Avg. Accumulated Reward')\n",
    "    \n",
    "    ax[0][1].plot([e for e in range(len(avg_accumulated_rewards))], avg_accumulated_rewards)\n",
    "    ax[0][1].set_xlabel('Episodes')\n",
    "    ax[0][1].set_ylabel('Avg. Accumulated Reward [log]')\n",
    "    ax[0][1].set_yscale('log')\n",
    "    \n",
    "    ax[1][0].plot([e for e in range(len(avg_value_net_loss))], avg_value_net_loss)\n",
    "    ax[1][0].set_xlabel('Episodes')\n",
    "    ax[1][0].set_ylabel('Avg. Value net loss')\n",
    "    \n",
    "    ax[1][1].plot([e for e in range(len(avg_value_net_loss))], avg_value_net_loss)\n",
    "    ax[1][1].set_xlabel('Episodes')\n",
    "    ax[1][1].set_ylabel('Avg. Value net loss [log]')\n",
    "    ax[1][1].set_yscale('log')\n",
    "    \n",
    "    ax[2][0].plot([e for e in range(len(avg_policy_net_loss))], avg_policy_net_loss)\n",
    "    ax[2][0].set_xlabel('Episodes')\n",
    "    ax[2][0].set_ylabel('Avg. Policy net loss')\n",
    "    \n",
    "    ax[2][1].plot([e for e in range(len(avg_policy_net_loss))], avg_policy_net_loss)\n",
    "    ax[2][1].set_xlabel('Episodes')\n",
    "    ax[2][1].set_ylabel('Avg. Policy net loss [log]')\n",
    "    ax[2][1].set_yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "elif architecture == \"Multi Head Network\":\n",
    "    fig, ax = plt.subplots(2,2, figsize=(12,10))\n",
    "    ax[0][0].plot([e for e in range(len(avg_accumulated_rewards))], avg_accumulated_rewards)\n",
    "    ax[0][0].set_xlabel('Episodes')\n",
    "    ax[0][0].set_ylabel('Avg. Accumulated Reward')\n",
    "    \n",
    "    ax[0][1].plot([e for e in range(len(avg_accumulated_rewards))], avg_accumulated_rewards)\n",
    "    ax[0][1].set_xlabel('Episodes')\n",
    "    ax[0][1].set_ylabel('Avg. Accumulated Reward [log]')\n",
    "    ax[0][1].set_yscale('log')\n",
    "    \n",
    "    ax[1][0].plot([e for e in range(len(avg_multihead_net_loss))], avg_multihead_net_loss)\n",
    "    ax[1][0].set_xlabel('Episodes')\n",
    "    ax[1][0].set_ylabel('Avg. Multi-head net loss')\n",
    "    \n",
    "    ax[1][1].plot([e for e in range(len(avg_multihead_net_loss))], avg_multihead_net_loss)\n",
    "    ax[1][1].set_xlabel('Episodes')\n",
    "    ax[1][1].set_ylabel('Avg. Multi-head net loss [log]')\n",
    "    ax[1][1].set_yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-08T00:01:34.371434Z"
    }
   },
   "id": "7c16c60286408ffb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Trying to interact w. env. after training\n",
    "Agent.play(render=False, max_game_length=max_game_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-08T00:01:34.373199Z"
    }
   },
   "id": "ef1be68ab58d878f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Testing architecture w. common backbone network w. Policy and Value heads:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fb2865a3a6e8ec7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "architecture = \"Multi Head Network\"\n",
    "\n",
    "\n",
    "Agent = PPOAgent(env=env,\n",
    "                 state_space_size=input_dim,\n",
    "                 action_space_size=output_dim,\n",
    "                 gamma=gamma,\n",
    "                 lmbda=lmbda,\n",
    "                 epsilon=epsilon,\n",
    "                 smooting_const=smoothing_const,\n",
    "                 normalize_advantages=normalize_advantages,\n",
    "                 batch_size=batch_size,\n",
    "                 max_game_length=max_game_length,\n",
    "                 shuffle_batches = shuffle_batches,\n",
    "                 architecture=architecture,\n",
    "                 seed=rng_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T00:01:34.378567Z",
     "start_time": "2023-11-08T00:01:34.374737Z"
    }
   },
   "id": "5f657b130f7bfd78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Trying to interact w. env. before training\n",
    "Agent.play(render=False, max_game_length=max_game_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-08T00:01:34.376488Z"
    }
   },
   "id": "32b5d84ee6aa35bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if architecture == \"Individual Networks\":\n",
    "    avg_accumulated_rewards, avg_value_net_loss, avg_policy_net_loss = Agent.train(episodes=num_episodes,\n",
    "                                                                                   policy_lr=policy_lr,\n",
    "                                                                                   value_lr=value_lr,\n",
    "                                                                                   num_policy_epochs=num_policy_epochs,\n",
    "                                                                                   num_value_epochs=num_value_epochs)\n",
    "elif architecture == \"Multi Head Network\":\n",
    "    avg_accumulated_rewards, avg_multihead_net_loss = Agent.train(episodes=num_episodes,\n",
    "                                                                  multihead_lr=multihead_lr,\n",
    "                                                                  num_multihead_epochs=num_multihead_epochs)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-08T00:01:34.378251Z"
    }
   },
   "id": "32497dac59aa5e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if architecture == \"Individual Networks\":\n",
    "    fig, ax = plt.subplots(3,2, figsize=(12,10))\n",
    "    ax[0][0].plot([e for e in range(len(avg_accumulated_rewards))], avg_accumulated_rewards)\n",
    "    ax[0][0].set_xlabel('Episodes')\n",
    "    ax[0][0].set_ylabel('Avg. Accumulated Reward')\n",
    "    \n",
    "    ax[0][1].plot([e for e in range(len(avg_accumulated_rewards))], avg_accumulated_rewards)\n",
    "    ax[0][1].set_xlabel('Episodes')\n",
    "    ax[0][1].set_ylabel('Avg. Accumulated Reward [log]')\n",
    "    ax[0][1].set_yscale('log')\n",
    "    \n",
    "    ax[1][0].plot([e for e in range(len(avg_value_net_loss))], avg_value_net_loss)\n",
    "    ax[1][0].set_xlabel('Episodes')\n",
    "    ax[1][0].set_ylabel('Avg. Value net loss')\n",
    "    \n",
    "    ax[1][1].plot([e for e in range(len(avg_value_net_loss))], avg_value_net_loss)\n",
    "    ax[1][1].set_xlabel('Episodes')\n",
    "    ax[1][1].set_ylabel('Avg. Value net loss [log]')\n",
    "    ax[1][1].set_yscale('log')\n",
    "    \n",
    "    ax[2][0].plot([e for e in range(len(avg_policy_net_loss))], avg_policy_net_loss)\n",
    "    ax[2][0].set_xlabel('Episodes')\n",
    "    ax[2][0].set_ylabel('Avg. Policy net loss')\n",
    "    \n",
    "    ax[2][1].plot([e for e in range(len(avg_policy_net_loss))], avg_policy_net_loss)\n",
    "    ax[2][1].set_xlabel('Episodes')\n",
    "    ax[2][1].set_ylabel('Avg. Policy net loss [log]')\n",
    "    ax[2][1].set_yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "elif architecture == \"Multi Head Network\":\n",
    "    fig, ax = plt.subplots(2,2, figsize=(12,10))\n",
    "    ax[0][0].plot([e for e in range(len(avg_accumulated_rewards))], avg_accumulated_rewards)\n",
    "    ax[0][0].set_xlabel('Episodes')\n",
    "    ax[0][0].set_ylabel('Avg. Accumulated Reward')\n",
    "    \n",
    "    ax[0][1].plot([e for e in range(len(avg_accumulated_rewards))], avg_accumulated_rewards)\n",
    "    ax[0][1].set_xlabel('Episodes')\n",
    "    ax[0][1].set_ylabel('Avg. Accumulated Reward [log]')\n",
    "    ax[0][1].set_yscale('log')\n",
    "    \n",
    "    ax[1][0].plot([e for e in range(len(avg_multihead_net_loss))], avg_multihead_net_loss)\n",
    "    ax[1][0].set_xlabel('Episodes')\n",
    "    ax[1][0].set_ylabel('Avg. Multi-head net loss')\n",
    "    \n",
    "    ax[1][1].plot([e for e in range(len(avg_multihead_net_loss))], avg_multihead_net_loss)\n",
    "    ax[1][1].set_xlabel('Episodes')\n",
    "    ax[1][1].set_ylabel('Avg. Multi-head net loss [log]')\n",
    "    ax[1][1].set_yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-08T00:01:34.379280Z"
    }
   },
   "id": "f0173c9d3592e585"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Trying to interact w. env. after training\n",
    "Agent.play(render=False, max_game_length=max_game_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-08T00:01:34.380363Z"
    }
   },
   "id": "773447a5a5375106"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-08T00:01:34.381475Z"
    }
   },
   "id": "d61083557b84e12a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
