{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:30<00:00,  6.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class PolicyNetwork(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 state_space_size: int,\n",
    "                 action_space_size: int,\n",
    "                 dtype: torch.dtype = torch.float32,\n",
    "                 device: str = 'cpu') -> None:\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        self.lin_layer_1 = torch.nn.Linear(in_features=self.state_space_size,\n",
    "                                           out_features=2 * self.state_space_size,\n",
    "                                           bias=True,\n",
    "                                           dtype=self.dtype,\n",
    "                                           device=self.device)\n",
    "        self.activtion_1 = torch.nn.ReLU()\n",
    "\n",
    "        self.lin_layer_2 = torch.nn.Linear(in_features=self.lin_layer_1.out_features,\n",
    "                                           out_features=self.action_space_size,\n",
    "                                           bias=True,\n",
    "                                           dtype=self.dtype,\n",
    "                                           device=self.device)\n",
    "        self.activtion_2 = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.activtion_1(self.lin_layer_1(state))\n",
    "        out = self.activtion_2(self.lin_layer_2(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class ValueNetwork(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 state_space_size: int,\n",
    "                 dtype: torch.dtype = torch.float32,\n",
    "                 device: str = 'cpu') -> None:\n",
    "        super(ValueNetwork, self).__init__()\n",
    "\n",
    "        self.state_space_size = state_space_size\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        self.lin_layer_1 = torch.nn.Linear(in_features=self.state_space_size,\n",
    "                                           out_features=self.state_space_size,\n",
    "                                           bias=True,\n",
    "                                           dtype=self.dtype,\n",
    "                                           device=self.device)\n",
    "        self.activtion_1 = torch.nn.ReLU()\n",
    "\n",
    "        self.lin_layer_2 = torch.nn.Linear(in_features=self.lin_layer_1.out_features,\n",
    "                                           out_features=1,\n",
    "                                           bias=True,\n",
    "                                           dtype=self.dtype,\n",
    "                                           device=self.device)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.activtion_1(self.lin_layer_1(state))\n",
    "        out = self.lin_layer_2(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, \n",
    "                 env: gym.Env,\n",
    "                 state_space_size: int,\n",
    "                 action_space_size: int,\n",
    "                 gamma: float,\n",
    "                 lmbda: float,\n",
    "                 epsilon: float,\n",
    "                 smooting_const: float,\n",
    "                 normalize_advantages: bool = True,\n",
    "                 dtype: torch.dtype = torch.float32,\n",
    "                 device: str = 'cpu'):\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.epsilon = epsilon  \n",
    "        self.smoothing_constant = smooting_const\n",
    "        self.normalize_advantages = normalize_advantages\n",
    "        \n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        \n",
    "        self.value_net = ValueNetwork(state_space_size=self.state_space_size,\n",
    "                                      dtype=self.dtype,\n",
    "                                      device=self.device)\n",
    "        \n",
    "        self.policy_net = PolicyNetwork(state_space_size=self.state_space_size, \n",
    "                                        action_space_size=self.action_space_size,\n",
    "                                        dtype=self.dtype,\n",
    "                                        device=self.device)\n",
    "        \n",
    "        self.policy_net_OLD = PolicyNetwork(state_space_size=self.state_space_size, \n",
    "                                            action_space_size=self.action_space_size,\n",
    "                                            dtype=self.dtype,\n",
    "                                            device=self.device)\n",
    "        # Initialize to same weights as policy net\n",
    "        self.policy_net_OLD.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        \n",
    "    def get_normalized_advantages(self, advantages: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize the advantages by subtracting the mean and dividing by the standard deviation.\n",
    "        \n",
    "        The formula for normalization is given by:\n",
    "        \n",
    "            .. math:: A_{\\\\text{normalized}} = \\\\frac{A - A_{\\\\text{mean}}}{A_{\\\\text{std}} + \\\\text{smoothing const.}}\n",
    "    \n",
    "        Parameters:\n",
    "            \n",
    "        - advantages (torch.Tensor): A tensor of advantages to be normalized.\n",
    "        - smoothing_constant (float, optional): A small value added for numerical stability. Default is 1e-10.\n",
    "        \n",
    "        Returns:\n",
    "            \n",
    "        - torch.Tensor: A tensor of normalized advantages.\n",
    "        \"\"\"\n",
    "        \n",
    "        normalized_advantages = (advantages - advantages.mean()) / (torch.std(advantages) + self.smoothing_constant)\n",
    "        return normalized_advantages\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_GAE(self, deltas: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the Generalized Advantage Estimation (GAE) for a given sequence of deltas and optionally normalize the advantages.\n",
    "        \n",
    "        GAE provides a bias-variance tradeoff for estimating the advantage function. It computes \n",
    "        the advantage by taking a weighted average of n-step advantage estimators:\n",
    "        \n",
    "            .. math:: A_t = \\\\delta_t + (\\\\gamma \\\\lambda) \\\\delta_{t+1} + (\\\\gamma \\\\lambda)^2 \\\\delta_{t+2} + \\\\dots\n",
    "        \n",
    "        Where:\n",
    "        \n",
    "        - :math:`\\\\delta_t` is the temporal difference (TD) residual at time t.\n",
    "        - :math:`\\\\gamma` is the discount factor.\n",
    "        - :math:`\\\\lambda` is a hyperparameter that determines the weighting of future TD residuals.\n",
    "        \n",
    "        If normalization is requested, the advantages are normalized using:\n",
    "        \n",
    "            .. math:: A_{\\\\text{normalized}} = \\\\frac{A - \\\\overline{A}}{\\\\text{std}(A) + \\\\text{smoothing\\_constant}}\n",
    "        \n",
    "        Parameters:\n",
    "        - deltas (torch.Tensor): A sequence of TD residuals.\n",
    "        - gamma (float): Discount factor, typically in the range [0, 1].\n",
    "        - lambda_ (float): GAE hyperparameter, typically in the range [0, 1].\n",
    "        - normalize_advantages (bool, optional): If set to True, the advantages are normalized. Default is True.\n",
    "        - smoothing_constant (float, optional): A small value added for numerical stability during normalization. Default is 1e-10.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: A tensor of computed (and possibly normalized) GAE advantages for each delta.\n",
    "        \"\"\"\n",
    "        \n",
    "        advantages = torch.zeros_like(deltas)\n",
    "        advantage = 0.0\n",
    "        for t in reversed(range(len(deltas))):\n",
    "            advantage = deltas[t] + self.gamma * self.lmbda * advantage\n",
    "            advantages[t] = advantage\n",
    "            \n",
    "        if self.normalize_advantages:\n",
    "            return self.get_normalized_advantages(advantages=advantages)\n",
    "        return advantages\n",
    "\n",
    "    def compute_TD_residual(self, reward_t: float, next_value_t: float, value_t: float) -> float:\n",
    "        \"\"\"\n",
    "        Compute the Temporal Difference (TD) residual for a given time step.\n",
    "        \n",
    "        The TD residual (  :math:`\\delta_t` ) is given by:\n",
    "        \n",
    "            .. math:: \\\\delta_t = r_t + \\\\gamma \\cdot V(s_{t+1}) - V(s_t)\n",
    "        \n",
    "        where:\n",
    "        \n",
    "        - :math:`r_t` is the reward at time :math:`t` .\n",
    "        - :math:`\\\\gamma` is the discount factor, typically in the range [0, 1] .\n",
    "        - :math:`V(s_t)` is the estimated value of state :math:`s_t` .\n",
    "        - :math:`V(s_{t+1})` is the estimated value of state :math:`s_{t+1}` .\n",
    "        \n",
    "        Parameters:\n",
    "            \n",
    "        - reward_t (float): Reward at time \\( t \\).\n",
    "        - gamma (float): Discount factor, typically in the range [0, 1].\n",
    "        - next_value_t (float): Estimated value of state at time \\( t+1 \\).\n",
    "        - value_t (float): Estimated value of state at time \\( t \\).\n",
    "        \n",
    "        Returns:\n",
    "            \n",
    "        - float: The computed TD residual for the given time step.\n",
    "        \"\"\"\n",
    "        \n",
    "        return reward_t + self.gamma * next_value_t - value_t\n",
    "\n",
    "\n",
    "    def get_policy_loss(self, state: torch.Tensor, action: int, advantage: float):\n",
    "        \"\"\"\n",
    "        Compute the Proximal Policy Optimization (PPO) clipped objective loss for a given state, action, and advantage.\n",
    "    \n",
    "        The PPO-Clip loss is defined as:\n",
    "    \n",
    "        .. math::\n",
    "            L^{\\\\text{CLIP}}(\\\\theta) = -\\\\mathbb{E}[\\\\min(r(\\\\theta) \\\\cdot A_t, \\\\text{clip}(r(\\\\theta), 1 - \\\\epsilon, 1 + \\\\epsilon) \\\\cdot A_t)]\n",
    "    \n",
    "        Where:\n",
    "    \n",
    "        - :math:`r(\\\\theta)` is the ratio of the probability of taking an action under the current policy to the probability under the old policy.\n",
    "        - :math:`A_t` is the advantage at time :math:`t`.\n",
    "        - :math:`\\\\epsilon` is a hyperparameter to clip the ratio.\n",
    "    \n",
    "        Parameters:\n",
    "            \n",
    "        - state (torch.Tensor): The state for which the policy loss is to be computed.\n",
    "        - action (int): The action taken by the agent.\n",
    "        - advantage (float): The computed advantage for the given state-action pair.\n",
    "        - epsilon (float): The hyperparameter for the PPO clipping.\n",
    "    \n",
    "        Returns:\n",
    "            \n",
    "        - float: The computed PPO-Clip loss for the given inputs.\n",
    "        \"\"\"\n",
    "        # Compute the PPO-Clip loss\n",
    "        \n",
    "        # Compute the probability of the action taken under the old policy\n",
    "        action_probs_old = self.policy_net_OLD(state)\n",
    "        pi_old = action_probs_old[action]\n",
    "        \n",
    "        # Compute the probability of the action taken under the current policy\n",
    "        action_probs_new = self.policy_net(state)\n",
    "        pi_new = action_probs_new[action]\n",
    "        \n",
    "        # Compute the ratio r(θ)\n",
    "        r = pi_new / pi_old\n",
    "        \n",
    "        # Compute the clipped surrogate objective\n",
    "        surrogate_obj = r * advantage\n",
    "        clipped_obj = torch.clamp(r, 1 - self.epsilon, 1 + self.epsilon) * advantage\n",
    "        \n",
    "        # Compute the PPO-Clip loss\n",
    "        loss = -torch.min(surrogate_obj, clipped_obj).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def get_value_loss(self, state: torch.Tensor, next_state: torch.Tensor, reward: torch.Tensor, is_last_step: bool):\n",
    "        \"\"\"\n",
    "        Compute the value loss for a given state using Temporal Difference (TD) learning.\n",
    "        \n",
    "        The value loss is calculated using the squared difference between the estimated value of the current state \n",
    "        and a target value. The target value is computed as:\n",
    "        \n",
    "            .. math:: \\\\text{target value} = r_t + \\\\gamma \\cdot V(s_{t+1}) \n",
    "            \n",
    "        Where:\n",
    "        \n",
    "        - :math:`r_t` is the reward at time :math:`t`.\n",
    "        - :math:`\\\\gamma` is the discount factor, typically in the range [0, 1].\n",
    "        - :math:`V(s_{t+1})` is the estimated value of state :math:`s_{t+1}`.\n",
    "        \n",
    "        Parameters:\n",
    "            \n",
    "        - state (torch.Tensor): The current state.\n",
    "        - next_state (torch.Tensor): The next state.\n",
    "        - reward (torch.Tensor): The reward at the current time step.\n",
    "        - gamma (float): Discount factor, typically in the range [0, 1].\n",
    "        - is_last_step (bool): Boolean indicating whether the current step is the last in the episode.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: The computed value loss for the given state.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute target value\n",
    "        if is_last_step:  # If it's the last step in the episode\n",
    "            target_value = reward\n",
    "        else:\n",
    "            # We detach the value estimate of the next state to prevent it from being \n",
    "            # updated during the gradient descent of the current state's value. \n",
    "            # This is done to treat the next state's value estimate as a constant target.\n",
    "            target_value = reward + self.gamma * self.value_net(next_state).detach()\n",
    "            \n",
    "        # Compute estimated value\n",
    "        value_estimate = self.value_net(state)\n",
    "        \n",
    "        # Compute the value loss\n",
    "        value_loss = nn.functional.mse_loss(value_estimate, target_value)\n",
    "\n",
    "        return value_loss\n",
    "    \n",
    "    \n",
    "    def train(self, episodes: int, policy_lr: float, value_lr: float, num_policy_epochs: int, num_value_epochs: int):\n",
    "        \n",
    "        # Define the optimizer for the policy network\n",
    "        policy_optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=policy_lr)\n",
    "        \n",
    "        # Define the optimizer for the value network\n",
    "        value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr=value_lr)\n",
    "        \n",
    "        accumulated_reward = []\n",
    "        \n",
    "        for episode in tqdm(range(episodes)):\n",
    "            # Retrieving initial state\n",
    "            state, info = self.env.reset()\n",
    "            \n",
    "            # For storing trajectory\n",
    "            states, actions, rewards, next_states = [], [], [], []\n",
    "            \n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                # Retrieve current action prob. distribution\n",
    "                action_probs = self.policy_net(torch.tensor(state))\n",
    "                \n",
    "                # Sample action from distribution\n",
    "                action = torch.multinomial(input=action_probs, num_samples=1)\n",
    "                \n",
    "                # interact with env.\n",
    "                next_state, reward, done, _, _ = self.env.step(action.item())\n",
    "                \n",
    "                # Store trajectory data\n",
    "                states.append(state.tolist())\n",
    "                actions.append([action.item()])\n",
    "                rewards.append([reward])\n",
    "                next_states.append(next_state.tolist())\n",
    "            \n",
    "                # Update current state\n",
    "                state = next_state\n",
    "            \n",
    "            # Making trajectory tensors to prepare for forward-pass in torch NN.    \n",
    "            states, actions, rewards, next_states = torch.tensor(states), torch.tensor(actions), torch.tensor(rewards), torch.tensor(next_states)\n",
    "            \n",
    "            # Saving game length in variable\n",
    "            game_length = states.shape[0]\n",
    "            \n",
    "            # Iterate backwards through the trajectory to compute deltas and advantages\n",
    "            deltas = torch.zeros(size=(game_length,))\n",
    "            for t in range(game_length):\n",
    "                # Retrieve data for current time step\n",
    "                state_t, next_state_t, reward_t = states[t], next_states[t], rewards[t]\n",
    "                \n",
    "                # Compute value estimates\n",
    "                value_t = self.value_net(state_t)\n",
    "                if t == game_length - 1:  # If it's the last step in the episode\n",
    "                    next_value_t = torch.tensor([[0.0]])  # The value is 0 at the end of the episode\n",
    "                else:\n",
    "                    next_value_t = self.value_net(next_state_t)\n",
    "            \n",
    "                # Compute the TD residual (delta)\n",
    "                deltas[t] = self.compute_TD_residual(reward_t=reward_t, next_value_t=next_value_t, value_t=value_t)\n",
    "            \n",
    "            advantages = self.compute_GAE(deltas=deltas)\n",
    "            \n",
    "            # Store the old policy parameters (before update)\n",
    "            self.policy_net_OLD.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "            # For a fixed number of policy update epochs:\n",
    "            for policy_epoch in range(num_policy_epochs):\n",
    "                \n",
    "                # TODO: Shuffle your data if needed (e.g., if you use mini-batches)\n",
    "                \n",
    "                for t in range(game_length):\n",
    "                    # Retrieve t'th step of trajectory\n",
    "                    state_t, action_t, advantage_t = states[t].detach(), actions[t].detach(), advantages[t].detach()\n",
    "                            \n",
    "                    # Compute the policy loss\n",
    "                    policy_loss = self.get_policy_loss(state=state_t, action=action_t, advantage=advantage_t)\n",
    "                    \n",
    "                    # Update policy parameters using the optimizer\n",
    "                    policy_optimizer.zero_grad()\n",
    "                    policy_loss.backward()\n",
    "                    policy_optimizer.step()\n",
    "                    \n",
    "            # Step 4: Value Network Update\n",
    "            for value_epoch in range(num_value_epochs):\n",
    "                \n",
    "                # TODO: Shuffle your data if needed (e.g., if you use mini-batches)\n",
    "                \n",
    "                value_losses = []  # To store value losses for debugging/analysis\n",
    "                \n",
    "                is_last_step=False\n",
    "                for t in range(game_length):\n",
    "                    # Retrieve t'th step of trajectory\n",
    "                    state_t, next_state_t, reward_t = states[t], next_states[t], rewards[t]\n",
    "                    if t == game_length - 1:\n",
    "                        is_last_step = True\n",
    "                    # Compute value loss\n",
    "                    value_loss = self.get_value_loss(state=state_t,\n",
    "                                                     next_state=next_state_t,\n",
    "                                                     reward=reward_t,\n",
    "                                                     is_last_step=is_last_step)\n",
    "                    \n",
    "                    # Update value network parameters using the optimizer\n",
    "                    value_optimizer.zero_grad()\n",
    "                    value_loss.backward()\n",
    "                    value_optimizer.step()\n",
    "            accumulated_reward.append(float(torch.sum(rewards).detach().numpy()))\n",
    "        return accumulated_reward\n",
    "\n",
    "\n",
    "# THE GAME: https://www.gymlibrary.dev/environments/classic_control/cart_pole/#rewards\n",
    "env = gym.make('CartPole-v1')\n",
    "env.action_space.seed(42)\n",
    "\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "gamma = 0.99\n",
    "lmbda = 0.925\n",
    "epsilon = 0.2  \n",
    "policy_lr = 0.001\n",
    "value_lr = 0.001\n",
    "num_policy_epochs = 4\n",
    "num_value_epochs = 4\n",
    "smoothing_const = 1e-8\n",
    "normalize_advantages = True\n",
    "\n",
    "Agent = PPOAgent(env=env,\n",
    "                 state_space_size=input_dim,\n",
    "                 action_space_size=output_dim,\n",
    "                 gamma=gamma,\n",
    "                 lmbda=lmbda,\n",
    "                 epsilon=epsilon,\n",
    "                 smooting_const=smoothing_const,\n",
    "                 normalize_advantages=normalize_advantages)\n",
    "\n",
    "num_episodes = 200\n",
    "accumulated_reward = Agent.train(episodes=num_episodes,\n",
    "                                 policy_lr=policy_lr,\n",
    "                                 value_lr=value_lr,\n",
    "                                 num_policy_epochs=num_policy_epochs,\n",
    "                                 num_value_epochs=num_value_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T00:04:13.341479Z",
     "start_time": "2023-11-02T00:03:42.595404Z"
    }
   },
   "id": "146bb736d5675521"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,3))\n",
    "ax.plot([e for e in range(num_episodes)], accumulated_reward)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T00:04:56.309390Z",
     "start_time": "2023-11-02T00:04:53.690384Z"
    }
   },
   "id": "21e910255a3048c9"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T00:04:15.805477Z",
     "start_time": "2023-11-02T00:04:15.799159Z"
    }
   },
   "id": "b3e41d2297ccb603"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
