{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:06:11.519787Z",
     "start_time": "2023-11-06T23:06:11.514714Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "from src2.ValueNetwork import ValueNetwork\n",
    "from src2.PolicyNetwork import PolicyNetwork\n",
    "from src2.Trajectories import Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self,\n",
    "                 env: gym.Env,\n",
    "                 state_space_size: int,\n",
    "                 action_space_size: int,\n",
    "                 batch_size: int,\n",
    "                 gamma: float,\n",
    "                 lmbda: float,\n",
    "                 epsilon: float,\n",
    "                 smooting_const: float,\n",
    "                 normalize_advantages: bool = True,\n",
    "                 dtype: torch.dtype = torch.float32,\n",
    "                 device: str = 'cpu'):\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.epsilon = epsilon\n",
    "        self.smoothing_constant = smooting_const\n",
    "        self.normalize_advantages = normalize_advantages\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        self.value_net = ValueNetwork(state_space_size=self.state_space_size,\n",
    "                                      dtype=self.dtype,\n",
    "                                      device=self.device)\n",
    "\n",
    "        self.policy_net = PolicyNetwork(state_space_size=self.state_space_size,\n",
    "                                        action_space_size=self.action_space_size,\n",
    "                                        dtype=self.dtype,\n",
    "                                        device=self.device)\n",
    "\n",
    "        self.policy_net_OLD = PolicyNetwork(state_space_size=self.state_space_size,\n",
    "                                            action_space_size=self.action_space_size,\n",
    "                                            dtype=self.dtype,\n",
    "                                            device=self.device)\n",
    "        # Initialize to same weights as policy net\n",
    "        self.policy_net_OLD.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def get_normalized_advantages(self, advantages: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        normalized_advantages = (advantages - advantages.mean()) / (torch.std(advantages) + self.smoothing_constant)\n",
    "        return normalized_advantages\n",
    "\n",
    "    def compute_GAE(self, deltas: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        advantages = torch.zeros_like(deltas)\n",
    "        advantage = 0.0\n",
    "        for t in reversed(range(len(deltas))):\n",
    "            advantage = deltas[t] + self.gamma * self.lmbda * advantage\n",
    "            advantages[t] = advantage\n",
    "\n",
    "        if self.normalize_advantages:\n",
    "            return self.get_normalized_advantages(advantages=advantages)\n",
    "        return advantages\n",
    "\n",
    "    def compute_TD_residual(self, reward_t: float, next_value_t: float, value_t: float) -> float:\n",
    "\n",
    "        return reward_t + self.gamma * next_value_t - value_t\n",
    "\n",
    "    def get_policy_loss(self, state: torch.Tensor, action: int, advantage: float):\n",
    "\n",
    "        # Compute the probability of the action taken under the old policy\n",
    "        action_probs_old = self.policy_net_OLD(state)\n",
    "        pi_old = action_probs_old[action]\n",
    "\n",
    "        # Compute the probability of the action taken under the current policy\n",
    "        action_probs_new = self.policy_net(state)\n",
    "        pi_new = action_probs_new[action]\n",
    "\n",
    "        # Compute the ratio r(Î¸)\n",
    "        r = pi_new / pi_old\n",
    "\n",
    "        # Compute the clipped surrogate objective\n",
    "        surrogate_obj = r * advantage\n",
    "        clipped_obj = torch.clamp(r, 1 - self.epsilon, 1 + self.epsilon) * advantage\n",
    "\n",
    "        # Compute the PPO-Clip loss\n",
    "        loss = -torch.min(surrogate_obj, clipped_obj).mean()\n",
    "        return loss\n",
    "\n",
    "    def get_value_loss(self, state: torch.Tensor, next_state: torch.Tensor, reward: torch.Tensor, is_last_step: bool):\n",
    "\n",
    "        # Compute target value\n",
    "        if is_last_step:  # If it's the last step in the episode\n",
    "            target_value = reward\n",
    "        else:\n",
    "            # We detach the value estimate of the next state to prevent it from being\n",
    "            # updated during the gradient descent of the current state's value.\n",
    "            # This is done to treat the next state's value estimate as a constant target.\n",
    "            target_value = reward + self.gamma * self.value_net(next_state).detach()\n",
    "\n",
    "        # Compute estimated value\n",
    "        value_estimate = self.value_net(state)\n",
    "\n",
    "        # Compute the value loss\n",
    "        value_loss = torch.nn.functional.mse_loss(value_estimate, target_value.reshape(value_estimate.shape))\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "    def train(self, episodes: int, policy_lr: float, value_lr: float, num_policy_epochs: int, num_value_epochs: int):\n",
    "\n",
    "        # Define the optimizer for the policy network\n",
    "        policy_optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "        # Define the optimizer for the value network\n",
    "        value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr=value_lr)\n",
    "\n",
    "        avg_accumulated_reward = []\n",
    "\n",
    "        for episode in tqdm(range(episodes)):\n",
    "            # Retrieving batch of trajectories\n",
    "            trajectories = Trajectories(batch_size=self.batch_size, env=self.env, policy_network=self.policy_net)\n",
    "            states_batch, actions_batch, rewards_batch, next_states_batch = trajectories.get_batch()\n",
    "\n",
    "            # Saving game length and batch size in variable\n",
    "            game_length = actions_batch.shape[1]\n",
    "\n",
    "            advantages_batch = []\n",
    "\n",
    "            for batch in range(self.batch_size):\n",
    "\n",
    "                states = states_batch[batch]\n",
    "                rewards = rewards_batch[batch]\n",
    "                next_states = next_states_batch[batch]\n",
    "\n",
    "                # Iterate backwards through the trajectory to compute deltas and advantages\n",
    "                deltas = torch.zeros(size=(game_length,))\n",
    "                for t in range(game_length):\n",
    "\n",
    "                    # Retrieve data for current time step\n",
    "                    state_t, next_state_t, reward_t = states[t], next_states[t], rewards[t]\n",
    "\n",
    "                    # Compute value estimates\n",
    "                    value_t = self.value_net(state_t)\n",
    "                    if t == game_length - 1:  # If it's the last step in the episode\n",
    "                        next_value_t = torch.tensor([[0.0]])  # The value is 0 at the end of the episode\n",
    "                    else:\n",
    "                        next_value_t = self.value_net(next_state_t)\n",
    "\n",
    "                    # Compute the TD residual (delta)\n",
    "                    deltas[t] = self.compute_TD_residual(reward_t=reward_t, next_value_t=next_value_t, value_t=value_t)\n",
    "\n",
    "                advantages_batch.append(self.compute_GAE(deltas=deltas))\n",
    "\n",
    "            # Store the old policy parameters (before update)\n",
    "            self.policy_net_OLD.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "            # For a fixed number of policy update epochs:\n",
    "            for policy_epoch in range(num_policy_epochs):\n",
    "\n",
    "                for batch in range(self.batch_size):\n",
    "                    states = states_batch[batch]\n",
    "                    actions = actions_batch[batch]\n",
    "                    advantages = advantages_batch[batch]\n",
    "\n",
    "                    for t in range(game_length):\n",
    "                        # Retrieve t'th step of trajectory\n",
    "                        state_t, action_t, advantage_t = states[t].detach(), actions[t].detach(), advantages[t].detach()\n",
    "\n",
    "                        # Compute the policy loss\n",
    "                        policy_loss = self.get_policy_loss(state=state_t, action=action_t, advantage=advantage_t)\n",
    "\n",
    "                        # Update policy parameters using the optimizer\n",
    "                        policy_optimizer.zero_grad()\n",
    "                        policy_loss.backward()\n",
    "                        policy_optimizer.step()\n",
    "\n",
    "            # Step 4: Value Network Update\n",
    "            for value_epoch in range(num_value_epochs):\n",
    "\n",
    "                for batch in range(self.batch_size):\n",
    "                    states = states_batch[batch]\n",
    "                    rewards = rewards_batch[batch]\n",
    "                    next_states = next_states_batch[batch]\n",
    "\n",
    "                    is_last_step = False\n",
    "                    for t in range(game_length):\n",
    "                        # Retrieve t'th step of trajectory\n",
    "                        state_t, next_state_t, reward_t = states[t], next_states[t], rewards[t]\n",
    "                        if t == game_length - 1:\n",
    "                            is_last_step = True\n",
    "                        # Compute value loss\n",
    "                        value_loss = self.get_value_loss(state=state_t,\n",
    "                                                         next_state=next_state_t,\n",
    "                                                         reward=reward_t,\n",
    "                                                         is_last_step=is_last_step)\n",
    "\n",
    "                        # Update value network parameters using the optimizer\n",
    "                        value_optimizer.zero_grad()\n",
    "                        value_loss.backward()\n",
    "                        value_optimizer.step()\n",
    "\n",
    "            avg_accumulated_reward.append(float(torch.mean(torch.sum(rewards_batch, dim=1)).detach().numpy()))\n",
    "        return avg_accumulated_reward\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a896effc7775110c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# THE GAME: https://www.gymlibrary.dev/environments/classic_control/cart_pole/#rewards\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:08:53.300701Z",
     "start_time": "2023-11-06T23:08:53.296148Z"
    }
   },
   "id": "1b2167c829413c68"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork(state_space_size=input_dim,\n",
    "                           action_space_size=output_dim)\n",
    "value_net = ValueNetwork(state_space_size=input_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:09:10.799072Z",
     "start_time": "2023-11-06T23:09:10.794272Z"
    }
   },
   "id": "85635900ef3a346c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "trajectories = Trajectories(env=env,batch_size=3,policy_network=policy_net)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:09:11.886579Z",
     "start_time": "2023-11-06T23:09:11.882979Z"
    }
   },
   "id": "7ec4cd4545678349"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "states_batch, actions_batch, rewards_batch, next_states_batch = trajectories.get_batch()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:10:26.083024Z",
     "start_time": "2023-11-06T23:10:26.072214Z"
    }
   },
   "id": "8294753d16b08149"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "states, actions, rewards, next_states = trajectories.get_trajectory()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:20:02.125238Z",
     "start_time": "2023-11-06T23:20:02.119792Z"
    }
   },
   "id": "26b6b894d82236ba"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "[-0.03350643068552017,\n -0.018029730767011642,\n -0.041487984359264374,\n 0.04232301935553551]"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:20:12.368526Z",
     "start_time": "2023-11-06T23:20:12.359365Z"
    }
   },
   "id": "919e99e2aa67067"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0328, -0.0125,  0.0177, -0.0261],\n        [-0.0331, -0.2079,  0.0172,  0.2721],\n        [-0.0373, -0.0130,  0.0226, -0.0151],\n        [-0.0375, -0.2085,  0.0223,  0.2846],\n        [-0.0417, -0.0137,  0.0280, -0.0009],\n        [-0.0420, -0.2092,  0.0280,  0.3004],\n        [-0.0461, -0.0145,  0.0340,  0.0167],\n        [-0.0464,  0.1802,  0.0343, -0.2651],\n        [-0.0428, -0.0154,  0.0290,  0.0383],\n        [-0.0431, -0.2110,  0.0298,  0.3400]])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_batch[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:13:20.673938Z",
     "start_time": "2023-11-06T23:13:20.669717Z"
    }
   },
   "id": "847925bbf86ad6b4"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0422, -0.0323, -0.0363,  0.0437],\n        [ 0.0415,  0.1633, -0.0354, -0.2602],\n        [ 0.0448, -0.0313, -0.0406,  0.0211],\n        [ 0.0442,  0.1644, -0.0402, -0.2841],\n        [ 0.0475,  0.3600, -0.0458, -0.5892],\n        [ 0.0547,  0.5558, -0.0576, -0.8960],\n        [ 0.0658,  0.7516, -0.0755, -1.2062],\n        [ 0.0808,  0.9476, -0.0997, -1.5216],\n        [ 0.0998,  1.1438, -0.1301, -1.8436],\n        [ 0.1226,  1.3401, -0.1670, -2.1737]])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_batch[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:13:26.409592Z",
     "start_time": "2023-11-06T23:13:26.405754Z"
    }
   },
   "id": "fcbed138aa141195"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-3.2849e-02, -1.2517e-02,  1.7686e-02, -2.6089e-02],\n        [-3.3099e-02, -2.0789e-01,  1.7165e-02,  2.7212e-01],\n        [-3.7257e-02, -1.3015e-02,  2.2607e-02, -1.5099e-02],\n        [-3.7517e-02, -2.0845e-01,  2.2305e-02,  2.8463e-01],\n        [-4.1686e-02, -1.3657e-02,  2.7998e-02, -9.3572e-04],\n        [-4.1959e-02, -2.0917e-01,  2.7979e-02,  3.0045e-01],\n        [-4.6142e-02, -1.4456e-02,  3.3988e-02,  1.6718e-02],\n        [-4.6432e-02,  1.8016e-01,  3.4322e-02, -2.6505e-01],\n        [-4.2828e-02, -1.5433e-02,  2.9021e-02,  3.8258e-02],\n        [-4.3137e-02, -2.1096e-01,  2.9786e-02,  3.3995e-01],\n        [ 4.2175e-02, -3.2334e-02, -3.6253e-02,  4.3656e-02],\n        [ 4.1528e-02,  1.6329e-01, -3.5380e-02, -2.6024e-01],\n        [ 4.4794e-02, -3.1311e-02, -4.0584e-02,  2.1077e-02],\n        [ 4.4168e-02,  1.6437e-01, -4.0163e-02, -2.8413e-01],\n        [ 4.7455e-02,  3.6004e-01, -4.5845e-02, -5.8920e-01],\n        [ 5.4656e-02,  5.5577e-01, -5.7629e-02, -8.9597e-01],\n        [ 6.5772e-02,  7.5163e-01, -7.5549e-02, -1.2062e+00],\n        [ 8.0804e-02,  9.4764e-01, -9.9673e-02, -1.5216e+00],\n        [ 9.9757e-02,  1.1438e+00, -1.3010e-01, -1.8436e+00],\n        [ 1.2263e-01,  1.3401e+00, -1.6698e-01, -2.1737e+00],\n        [-1.4497e-02, -4.0444e-02, -8.6454e-03,  2.5871e-02],\n        [-1.5306e-02, -2.3544e-01, -8.1280e-03,  3.1581e-01],\n        [-2.0015e-02, -4.3045e-01, -1.8118e-03,  6.0592e-01],\n        [-2.8624e-02, -2.3530e-01,  1.0307e-02,  3.1267e-01],\n        [-3.3330e-02, -4.0325e-02,  1.6560e-02,  2.3254e-02],\n        [-3.4136e-02, -2.3568e-01,  1.7025e-02,  3.2112e-01],\n        [-3.8850e-02, -4.3104e-01,  2.3447e-02,  6.1912e-01],\n        [-4.7471e-02, -2.3625e-01,  3.5830e-02,  3.3391e-01],\n        [-5.2196e-02, -4.1660e-02,  4.2508e-02,  5.2740e-02],\n        [-5.3029e-02, -2.3736e-01,  4.3563e-02,  3.5853e-01]])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = states_batch.reshape((states_batch.shape[0]*states_batch.shape[1],states_batch.shape[2]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:16:05.162042Z",
     "start_time": "2023-11-06T23:16:05.157484Z"
    }
   },
   "id": "a34aaa640e23c163"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.2931],\n        [0.3156],\n        [0.2940],\n        [0.3170],\n        [0.2954],\n        [0.3177],\n        [0.2971],\n        [0.3391],\n        [0.2950],\n        [0.3157],\n        [0.2764],\n        [0.2969],\n        [0.2755],\n        [0.2985],\n        [0.3439],\n        [0.3861],\n        [0.4255],\n        [0.4547],\n        [0.4681],\n        [0.4798],\n        [0.2869],\n        [0.3105],\n        [0.2916],\n        [0.3146],\n        [0.2953],\n        [0.3152],\n        [0.2939],\n        [0.3177],\n        [0.3033],\n        [0.3176]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas = torch.zeros(size=(game_length,))\n",
    "for t in range(game_length):\n",
    "\n",
    "    # Retrieve data for current time step\n",
    "    state_t, next_state_t, reward_t = states[t], next_states[t], rewards[t]\n",
    "\n",
    "    # Compute value estimates\n",
    "    value_t = self.value_net(state_t)\n",
    "    if t == game_length - 1:  # If it's the last step in the episode\n",
    "        next_value_t = torch.tensor([[0.0]])  # The value is 0 at the end of the episode\n",
    "    else:\n",
    "        next_value_t = self.value_net(next_state_t)\n",
    "\n",
    "    # Compute the TD residual (delta)\n",
    "    deltas[t] = self.compute_TD_residual(reward_t=reward_t, next_value_t=next_value_t, value_t=value_t)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:16:34.373353Z",
     "start_time": "2023-11-06T23:16:34.365480Z"
    }
   },
   "id": "751f1a49cd663d60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8afcb6a0061baad5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
